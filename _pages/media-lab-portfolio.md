---
title: "Media Lab–Focused Portfolio"
permalink: /media-lab/
---

This page curates a subset of my work most aligned with **Media Lab-style research**:  
**novel sensing, wearable computing, personal informatics, and human-centered AI.**

## 1. Acoustic Input for Headless Wearables

A new **acoustic interaction channel** for ring-like and screenless wearables using low-intensity sound and **MFCC + spectrogram modeling**.

- Runs on **commodity microphones**
- Robust to **noise and everyday environments**
- Designed for **subtle, eyes-free interaction**

[Project details »]({{ "/projects/acoustic-input-headless-wearables/" | relative_url }})

---

## 2. Activity-Based Health Insights on Galaxy Watch

A **multimodal health insight system** that fuses IMU, PPG, pressure, and contextual health data to recommend **activity-linked insights** to users.

- Supports **behavior reflection** and **daily health awareness**
- Demonstrated at the **Samsung AI POC Exhibition**
- Built to run in consumer-scale wearable ecosystems

[Project details »]({{ "/projects/activity-health-insights/" | relative_url }})

---

## 3. Experimenting with Behavior & Focus Modeling

Additional experimental work (e.g., **EEG–vision-based focus detection**, homonym disambiguation based on prosody) explores how **subtle signals** can reveal cognitive states and support new interaction paradigms.

While still early-stage, these experiments shape my long-term interest in:

- **Everyday cognitive augmentation**
- **Implicit sensing for attention, workload, and mental state**
- **Interventions that feel natural and respectful, not intrusive**

---

My long-term vision is to design **sensing-driven interfaces** that don’t just measure people, but **support them**—in understanding their bodies, habits, and minds, and in taking small, meaningful actions toward better health and well-being.
